---
title: Основной модуль обучения работе со смешанной реальностью. Расширенный ввод
description: В рамках этого курса вы узнаете, как реализовать функцию распознавания лиц Azure в приложении смешанной реальности.
author: jessemcculloch
ms.author: jemccull
ms.date: 02/26/2019
ms.topic: article
ms.localizationpriority: high
keywords: mixed reality, unity, tutorial, hololens
ms.openlocfilehash: 32141aafd43c5d729919673509c93bb2014edd37
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 06/05/2019
ms.locfileid: "66719897"
---
# <a name="mr-learning-base-module---advanced-input"></a>Основной модуль обучения работе со смешанной реальностью. Расширенный ввод

В этом уроке мы рассмотрим несколько вариантов расширенного ввода для HoloLens 2, в том числе голосовые команды, жесты сдвига и отслеживание взгляда. 

## <a name="objectives"></a>Задачи

- Узнать, как активировать события с помощью голосовых команд и ключевых слов.
- Научиться применять отслеживаемые руки для сдвига текстур и трехмерных объектов.
- Научиться использовать функцию отслеживания взгляда в HoloLens 2 для выбора объектов.

## <a name="instructions"></a>Инструкция

### <a name="enabling-voice-commands"></a>Включение голосовых команд

В этом разделе мы реализуем поддержку двух голосовых команд. Во-первых, возможность включить и выключить панель диагностики частоты кадров с помощью голосовой команды "toggle diagnostics" (переключить диагностику). Во-вторых, возможность воспроизводить звук с помощью голосовой команды. Для начала мы изучим профили и настройки MRTK, которые отвечают за настройку голосовых команд. 

1. В иерархии базовой сцены выберите элемент MixedRealityToolkit. На панели инспектора прокрутите вниз до параметров системы ввода. Дважды щелкните профиль системы ввода, чтобы открыть его. Клонируйте профиль системы ввода, чтобы получить доступ к его редактированию, как описано в [уроке 1](mrlearning-base-ch1.md). 

В профиле системы ввода вы увидите список параметров. Чтобы найти параметры голосовых команд, перейдите вниз на строку Speech Command Settings (Параметры голосовых команд). 

![Изображение "Урок 5, глава 1, шаг 2"](images/Lesson5_Chapter1_step2im.PNG)

2. Клонируйте профиль голосовых команд, чтобы получить доступ к его редактированию, как описано в [уроке 1](mrlearning-base-ch1.md). Дважды щелкните профиль голосовых команд, где вы найдете ряд параметров. Полное описание этих параметров см. в разделе [документации по речевым возможностям MRTK](<https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/Input/Speech.html>). 

>Примечание. По умолчанию используется автоматический запуск. Вы можете изменить его на запуск вручную, но в этом примере мы сохраним режим автоматического запуска. MRTK поставляется с несколькими стандартными голосовыми командами (например, menu, toggle diagnostics и toggle profiler). Мы будем использовать ключевое слово toggle diagnostics (переключить диагностику), которое позволяет включать и отключать счетчик частоты кадров диагностики. Также мы добавим новую голосовую команду, выполнив описанные ниже действия.
>
> ![Изображение "Урок 5, глава 1, примечание"](images/Lesson5_chapter1_noteim.PNG)

3. Добавьте новую голосовую команду. Для этого нажмите кнопку "+ add a new speech command". Появится новая строка под списком существующих голосовых команд. Введите здесь новую голосовую команду. Для этого примера музыкального приложения мы выбрали команду play music (воспроизводить музыку).

>Совет. Можно также задать код клавиши для голосовой команды. Это позволяет выполнять голосовые команды при нажатии клавиши на клавиатуре.   

4. Добавьте возможность реагировать на голосовые команды. Выберите любой объект в иерархии базовой сцены, к которому не подключены никакие скрипты ввода (такие как обработчик манипулирования). На панели инспектора нажмите кнопку Add Component (Добавить компонент). Введите "speech input handler" (обработчик речевого ввода). Выберите соответствующий элемент списка.
   ![Изображение "Урок 5, глава 1, шаг 4"](images/Lesson5_chapter1_step4im.PNG)

   

По умолчанию здесь отображаются два флажка, один из которых озаглавлен "is focus required" (требуется ли фокус). Это значит, что голосовая команда выполняется, только пока вы удерживаете луч управления на этом объекте (луч взгляда, направления головы, контроллера или руки). Снимите этот флажок, чтобы пользователю не нужно было смотреть на объект для использования голосовых команд.

5. Добавьте возможность реагировать на голосовую команду. Для этого нажмите кнопку "+" в обработчике голосового ввода и выберите ключевое слово, на которое вы хотите настроить реакцию.

   > Примечание. Эти ключевые слова заполняются из профиля, который вы редактировали на предыдущем шаге.

![Изображение "Урок 5, глава 1, шаг 5"](images/Lesson5_chapter1_step5im.PNG)

6. Рядом с полем Keyword (Ключевое слово) отображается раскрывающееся меню. Выберите Toggle Diagnostics (Переключить диагностику). Теперь это действие будет активироваться каждый раз, когда пользователь произносит фразу "toggle diagnostics" (переключить диагностику). Возможно, вам придется развернуть пункт Element 0 (Элемент 0), нажав стрелку рядом с ним.

![Изображение "Урок 5, глава 1, шаг 6"](images/Lesson5_chapter1_step6im.PNG)

7. Добавьте скрипт diagnostics demo control script (скрипт для демонстрационного элемента управления диагностикой) для переключения диагностического элемента со счетчиком частоты кадров. Для этого нажмите кнопку Add component (Добавить компонент) и выполните поиск по строке "diagnostics demo control script", а затем добавьте этот скрипт через меню. Его можно добавить к любому объекту, но для простоты мы применим тот же объект, где размещен обработчик голосового ввода. 

   > Примечание. Этот скрипт поставляется только в этих модулях и не входит в состав исходного набора MRTK.

![Изображение "Урок 5, глава 1, шаг 7"](images/Lesson5_chapter1_step7im.PNG)

8. Добавьте новый ответ в обработчике голосового ввода. Для этого щелкните кнопку "+" под надписью "response ()" (ответ ()), которая обозначена зеленой стрелкой на рисунке выше.

![Изображение "Урок 5, глава 1, шаг 7"](images/Lesson5_chapter1_step8.PNG)

9. Перетащите объект, содержащий скрипт Diagnostics Demo Controls (Демонстрационные элементы управления диагностикой), в новый ответ, который вы создали на шаге 8.
    ![Изображение "Урок 5, глава 1, шаг 9"](images/Lesson5_chapter1_step9im.PNG)

10. Теперь выберите в раскрывающемся списке no function (нет функции) демонстрационные элементы управления диагностикой, а затем элемент on toggle diagnostics (). Эта функция включает и выключает диагностику.  ![Изображение "Урок 5, глава 1, шаг 10"](images/Lesson5_chapter1_step10im.PNG)
    
> Обратите внимание на то, что перед сборкой приложения для устройства необходимо включить параметры микрофона. Для этого щелкните файл, перейдите к параметрам сборки, затем откройте параметры проигрывателя и убедитесь, что здесь включена возможность microphone (микрофон).

Теперь мы добавим возможность воспроизводить звуковой файл с помощью голосовой команды через объект Octa. Как вы помните, в [уроке 4](mrlearning-base-ch4.md) мы добавили возможность воспроизводить аудиоклип касанием объекта Octa. Теперь мы применим тот же аудиоисточник для голосовой команды воспроизведения музыки.

11. Выберите объект Octa в иерархии базовой сцены.

12. Добавьте еще один обработчик голосового ввода (повторите шаги 4 и 5), но теперь для объекта Octa. 

13. На шаге 6 вместо голосовой команды toggle diagnostics (переключить диагностику) добавьте голосовую команду play music (воспроизводить музыку), как показано на рисунке ниже.
    
     ![Изображение "Урок 5, глава 1, шаг 13"](images/Lesson5_chapter1_step13im.PNG)
    
    
    
14. Как и на шагах 8 и 9, добавьте новый ответ и перетащите объект Octa в пустой слот в этом ответе.

15. Выберите раскрывающееся меню с надписью no function (без функции), выберите элемент Audio Source (Источник звука), а затем — PlayOneShot (AudioClip) (Воспроизвести один кадр (аудиоклип)).

![Изображение "Урок 5, глава 1, шаг 15"](images/Lesson5_chapter1_step15im.PNG)

16. Для нашего примера мы выберем тот же аудиоклип, что и в [уроке 4](mrlearning-base-ch4.md). Перейдите на панель проекта, найдите аудиоклип MRTK_Gem и перетащите его в слот аудиоисточника, как показано на рисунке ниже. Теперь ваше приложение сможет отвечать на голосовые команды toggle diagnostics (переключить диагностику), которая переключает панель счетчика частоты кадров, и play music (воспроизводить музыку), которая запускает песню MRTK_Gem.
     ![Изображение "Урок 5, глава 1, шаг 16"](images/Lesson5_chapter1_step16im.PNG)


### <a name="the-pan-gesture"></a>Жест сдвига 

Из этой главы вы узнаете, как использовать жест сдвига. Он полезен для прокрутки содержимого жестом пальца или руки. Также вы можете использовать жест сдвига для вращения объектов, пролистывания коллекций трехмерных объектов или даже для прокрутки двухмерных пользовательских интерфейсов. Сейчас мы научим вас использовать жест сдвига для сгибания текстуры. Мы также узнаем, как перемещать коллекцию трехмерных объектов.

1. Создайте объект Quad. В иерархии базовой сцены щелкните правой кнопкой мыши, выберите 3D Object (Трехмерный объект), а затем — Quad.

![Изображение "Урок 5, глава 2, шаг 2"](images/Lesson5_chapter2_step2im.PNG)

2. Поместите Quad в любое удобное место. В нашем примере указаны параметры x=0, y=0 и z=1,5, чтобы настроить удобное положение относительно HoloLens 2.

   > Примечание. Если объект Quad блокирует (загораживает) какое-либо содержимое из предыдущих занятий, переместите его так, чтобы устранить подобные помехи.

3. Примените материал к объекту Quad. Именно этот материал мы будет прокручивать жестом сдвига. 

![Изображение "Урок 5, глава 2, шаг 3"](images/Lesson5_chapter2_step3im.PNG)

4. На панели проектов введите в поле поиска строку "pan content" (сдвиг содержимого). Перетащите этот материал на объект Quad, размещенный в сцене. 

> Примечание. Материал Pan content (Сдвиг содержимого) не входит в состав MRTK, а включен в виде ресурса в пакет ресурсов этого модуля, который мы импортировали в предыдущих уроках. 

> Примечание. Когда вы добавите элемент Pan content (Сдвиг содержимого), он может выглядеть растянутым. Это можно исправить, настроив для объекта Quad значения размера x, y и z так, чтобы его внешний вид вас устроил.

Чтобы использовать жест сдвига, для объекта нужно добавить коллайдер. Возможно, вы заметили, что объект Quad уже имеет сетчатый коллайдер. Но этот вариант нам подходит плохо, так как он очень тонкий и его трудно выделять. Мы рекомендуем заменить сетчатый коллайдер на прямоугольный.

5. Щелкните правой кнопкой мыши сетчатый коллайдер объекта Quad (на панели инспектора), а затем удалите его кнопкой Remove component (Удалить компонент). 
   ![Изображение "Урок 5, глава 2, шаг 5"](images/Lesson5_chapter2_step5im.PNG)

6. Теперь добавьте прямоугольный коллайдер, щелкнув кнопку Add component (Добавить компонент) и выполнив поиск по строке "box collider" (прямоугольный коллайдер). Но созданный коллайдер по умолчанию все еще слишком тонок. Нажмите кнопку Edit collider (Редактировать коллайдер), чтобы изменить его. Пока эта кнопка остается в нажатом состоянии, вы можете изменять значения размера x, y и z для любых элементов в редакторе сцен. В нашем примере мы хотим расширить прямоугольный коллайдер, чтобы он немного выходил за пределы объекта Quad. В редакторе сцен перетащите прямоугольный коллайдер с обратной стороны наружу (как на рисунке ниже). Это действие позволит пользователю использовать для прокрутки не только палец, но и всю руку. 
    ![Изображение "Урок 5, глава 2, шаг 6"](images/Lesson5_chapter2_step6im.PNG)
7. Добавьте интерактивность. Поскольку мы хотим напрямую взаимодействовать с объектом Quad, следует применить к нему компонент near interaction touchable (касание при ближнем взаимодействии), который мы уже использовали в уроке 4 для воспроизведения музыки через объект Octa. Нажмите кнопку Add Component (Добавить компонент) и найдите элемент near interaction touchable (касание при ближнем взаимодействии), как показано на рисунке ниже. 

8. Добавьте возможность распознавания жеста сдвига. Нажмите кнопку Add component (Добавить компонент) и введите "hand interaction pan" (сдвиг при взаимодействии с рукой). Вы можете выбрать здесь луч руки (чтобы выполнять сдвиг на расстоянии) или указательный палец. Для нашего примера давайте оставим указательный палец. 
    ![Изображение "Урок 5, глава 2, шаги 7 и 8"](images/Lesson5_chapter2_step7-8im.PNG)

![Изображение "Урок 5, глава 2, шаг 8"](images/Lesson5_chapter2_step8im.PNG)

9. В скрипте сдвига при взаимодействии с рукой есть флажки lock horizontal (блокировка по горизонтали) и lock vertical (блокировка по вертикали), которые запрещают соответствующие перемещения. Параметр wrap texture (перенос текстуры) означает, что сопоставленная текстура перемещается в соответствии с жестами сдвига, которые выполняет пользователь. Для нашего примера мы установим этот флажок. Есть также флажок velocity active (скорость активна), при снятии которого жест сдвига не будет работать. Установите этот флажок. Теперь ваш объект Quad должен полностью поддерживать операцию сдвига.

   

   Теперь мы изучим сдвиг трехмерных объектов. 

10. Щелкните правой кнопкой мыши объект Quad, выберите элемент 3D Object (Трехмерный объект), а затем — Cube (Куб). Измените масштаб куба примерно до значений x=0,1, y=0,1 и z=0,1. Скопируйте этот куб три раза (щелкнув куб правой кнопкой мыши и выбрав действие дублирования либо нажав сочетание клавиш CTRL+D или COMMAND+D). Разместите копии равномерно. Теперь сцена должна выглядеть так, как показано на рисунке ниже.

![Изображение "Урок 5, глава 2, шаг 10"](images/Lesson5_chapter2_step10im.PNG)







11. Снова выберите объект Quad и в скрипте сдвига при взаимодействии с рукой настройте действия сдвига для каждого из этих кубов. В параметре Pan event receivers (Приемники событий сдвига) задайте число объектов, которые получают эти события. Поскольку мы создали четыре куба, введите число 4 и нажмите клавишу ВВОД. Появятся четыре пустых поля.


![Изображение "Урок 5, глава 2, шаг 11"](images/Lesson5_chapter2_step11im.PNG)



12. Перетащите каждый из кубов в пустые слоты элементов.
     ![Изображение "Урок 5, глава 2, шаг 12"](images/Lesson5_chapter2_step12im.PNG)
    
13. Добавьте для всех кубов скрипт move with pan (перемещать при сдвиге). Для этого выберите каждый объект поочередно, удерживая нажатой клавишу CTRL (или COMMAND). На панели инспектора нажмите кнопку Add Component (Добавить компонент) и выполните поиск по строке "move with pan". Выберите нужный скрипт, и он будет добавлен в каждый куб. Наши трехмерные объекты теперь перемещаются жестом сдвига! Если вы удалите отрисовку сетки для объекта Quad, он станет невидимым при прокручивании списка трехмерных объектов жестом сдвига.

### <a name="eye-tracking"></a>Отслеживание взгляда

В этой главе мы изучим, как включить отслеживание взгляда в нашем демонстрационном приложении. Мы будем медленно вращать элементы трехмерного меню, когда пользователь наводит на них взгляд. Мы также будем активировать специальный эффект при выборе элемента, на который наведен взгляд.

1. Убедитесь, что профили набора средств для смешанной реальности настроены правильно. На момент написания этой статьи в конфигурации профиля набора средств для смешанной реальности по умолчанию не включается отслеживание взгляда. Чтобы добавить возможности отслеживания взгляда, выполните инструкции из раздела о настройке профилей MRTK, необходимых для отслеживания взгляда, который включен в [документацию по набору средств для смешанной реальности](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_BasicSetup.html#setting-up-the-mrtk-profiles-required-for-eye-tracking  ). Убедитесь, что отслеживание взгляда настроено правильно, выполнив все оставшиеся действия из инструкции по указанной выше ссылке, в том числе включение отслеживания взгляда в GazeProvider (компонент, присоединенный к камере) и включение моделирования отслеживания взгляда в редакторе Unity. Обратите внимание, что в будущих версиях MRTK отслеживание взгляда может быть включено по умолчанию.

    По указанной выше ссылке вы найдете краткие инструкции по выполнению следующих действий:

    - создание поставщика данных о направлении взгляда для использования в профиле MRTK;
    - включение отслеживания взгляда в поставщике данных о направлении взгляда;
    - настройка имитации отслеживания взгляда в редакторе;
    - редактирование возможностей решения Visual Studio, чтобы разрешить отслеживание взгляда в скомпилированном приложении.

2. Добавьте компонент отслеживания взгляда в целевые объекты. Чтобы объекты могли реагировать на события взгляда, следует добавить к каждому из них компонент EyeTrackingTarget. Добавьте этот компонент в каждый из девяти трехмерных объектов, которые входят в коллекцию сетки. Совет. Выберите несколько элементов в иерархии сразу, чтобы добавить компонент EyeTrackingTarget в каждый из них.
    ![Урок 5, глава 3, шаг 2](images/Lesson5Chapter3Step2.JPG)

3. Теперь мы добавим скрипт EyeTrackingTutorialDemo, чтобы реализовать интересные возможности взаимодействия. Скрипт EyeTrackingTutorialDemo входит в репозиторий для этой серии руководств, но не поставляется по умолчанию с набором средств для смешанной реальности. К каждому трехмерному объекту в коллекции сетки добавьте скрипт EyeTrackingTutorialDemo, выполнив поиск компонента в меню Add Component (Добавить компонент).
   ![Урок 5, глава 3, шаг 3](images/Lesson5Chapter3Step3.JPG)

   4. Поверните объект, удерживая взгляд на целевом объекте. Мы хотим настроить трехмерный объект так, чтобы он вращался, пока на него смотрят. Для этого вставьте новое поле в раздел While Looking At Target (Пока взгляд остается на целевом объекте) для компонента EyeTrackingTarget, как показано на рисунке ниже. 

![Урок 5, глава 3, шаг 4a](images/Lesson5Chapter3Step4a.JPG)
![Урок 5, глава 3, шаг 4b](images/Lesson5Chapter3Step4b.JPG)



В только что созданном поле добавьте в пустое поле текущий игровой объект и выберите действие EyeTrackingTutorialDemo > RotateTarget(), как показано на рисунке ниже. Теперь трехмерный объект будет вращаться, когда система отслеживания взгляда определит, что пользователь смотрит на него. 

5. Добавьте возможность blip target (передать импульс целевому объекту) при выборе объекта (жестом касания или голосовой командой select (выбрать)). Как и на шаге 4, для включения возможности EyeTrackingTutorialDemo > BlipTarget() мы поместим ее в поле Select() игрового объекта для компонента EyeTrackingTarget, как показано на рисунке ниже. После этой настройки вы будете замечать небольшой импульс игрового объекта при активации действия выбора этого объекта, например жестом касания или голосовой командой select (выбрать). 
    ![Урок 5, глава 3, шаг 5](images/Lesson5Chapter3Step5.JPG)
6. Убедитесь, что возможности отслеживания взгляда правильно настроены, прежде чем выполнять компиляцию в HoloLens 2. На момент написания этой статьи в Unity отсутствовала возможность настраивать ввод взглядом (для отслеживания взгляда). Настройка этой возможности является обязательной для отслеживания взгляда в HoloLens 2. Выполните эти инструкции из документации по набору средств для смешанной реальности, чтобы включить возможность ввода взглядом: https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_BasicSetup.html#testing-your-unity-app-on-a-hololens-2 


### <a name="congratulations"></a>Поздравляем! 
Вы успешно добавили в приложение базовые возможности отслеживания взгляда. Эти действия только открывают целый мир новых возможностей, которые предоставляет отслеживание взгляда. Эта глава завершает урок 5, из которого мы узнали о таких расширенных способах ввода, как голосовые команды, жесты сдвига и отслеживание взгляда. 

[Следующий урок. Пример сборки лунного модуля](mrlearning-base-ch6.md)

